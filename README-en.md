# **Hands-on Learning: Transformer**

[ç®€ä½“ä¸­æ–‡](./README.md) | English

## **ğŸš€ Project Introduction**

This project reconstructs the Transformer architecture proposed in the paper "**Attention is All You Need**" from scratch. Through step-by-step code implementation and theoretical analysis, we deeply explore the design principles and implementation details of this revolutionary model.

## **ğŸ“š Background**

The Transformer model proposed in 2017 completely changed the paradigm of sequence modeling:

- âš¡ Entirely based on attention mechanisms, abandoning the temporal dependencies of RNN/CNN
- ğŸš„ Friendly to parallel computing, significantly improving training efficiency
- ğŸ—ï¸ Becoming the cornerstone of milestone models such as BERT and GPT

## **ğŸ§­ Learning Path**

| **Stage** | **Topic**                                    | **Status** |
| --------- | -------------------------------------------- | ---------- |
| Day 1     | ğŸ“¥ Input Representation & Positional Encoding | âœ…          |
| Day 2     | ğŸ‘ï¸ Self-Attention Mechanism                   | ğŸ”œ          |
| Day 3     | ğŸ” Encoder Module                             | ğŸ”œ          |
| Day 4     | ğŸ”® Decoder Module                             | ğŸ”œ          |
| Day 5     | ğŸ† Complete Transformer                       | ğŸ”œ          |

## **ğŸ” Day 1: Input Representation & Positional Encoding**

- Input Embedding Layer 

  - âœ¨ Learnable mapping from tokens to vectors
  - ğŸ”¢ Scaling factor `sqrt(d_model)`

- Positional Encoding 

  - ğŸ“ Implementation of sine/cosine positional encoding formula

  ```
  PE(pos,2i) = sin(pos/10000^(2i/d_model))
  PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
  ```

  - ğŸ§® Mathematical representation of positional information
  - ğŸ”„ Advantages of relative positional encoding

## **ğŸ‘ï¸ Day 2: Self-Attention Mechanism (ğŸ”œ Coming Soon)**

- Multi-Head Attention
  - ğŸ”„ Implementation of parallel attention heads
  - ğŸ”— Computation flow of attention matrices
- Scaled Dot-Product Attention
  - ğŸ“ Necessity of the scaling factor
  - ğŸ§® Numerical stability of Softmax

## **ğŸ”§ Day 3: Encoder Module (ğŸ”œ Coming Soon)**

- Feed-Forward Neural Network
  - ğŸ§  Implementation of position-wise feed-forward networks
  - ğŸ“ˆ Choice of activation functions
- Residual Connections & Layer Normalization
  - ğŸ”„ Importance of residual mechanisms
  - ğŸ“Š Theoretical foundation of normalization strategies

## **ğŸ”® Day 4: Decoder Module (ğŸ”œ Coming Soon)**

- Masked Self-Attention
  - ğŸ”’ Implementation of future information masking
  - ğŸ‘ï¸ Special characteristics of decoder attention
- Encoder-Decoder Attention
  - ğŸ”„ Cross-attention mechanism
  - ğŸ” Information flow of Query-Key-Value

## **ğŸ† Day 5: Complete Transformer (ğŸ”œ Coming Soon)**

- Model Integration
  - ğŸ§© Assembly of the complete architecture
  - ğŸ”„ Forward propagation process
- Training & Optimization
  - ğŸ“‰ Loss function design
  - ğŸ”§ Parameter initialization strategies

## **ğŸ“š References**

- ğŸ“‘ **Attention is All You Need**

## **ğŸ¤ Contribution Guidelines**

Contributions to this project are welcome!

- ğŸ› Found issues or have suggestions? Submit an issue
- ğŸ’¡ Want to improve code or documentation? Submit a PR
- ğŸ“š Share learning experiences or implementation ideas? Join the discussion

## **ğŸ“„ License**

MIT License

**If this project has been helpful to you, please give it a â­ï¸ to encourage the author to continue creating!**